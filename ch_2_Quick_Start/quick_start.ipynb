{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "This tutorial offers a step-by-step guide that allows for hands-on learning. We will begin by setting up required environment and then proceed to develop an application with BigDL-LLM transformers INT4 optimization. This application will allow us to conduct inferences on a large language model with low latency. By following this tutorial, you will gain a seamless experience that will enable you to easily comprehend and follow the upcoming tutorials.\n",
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Recommended System\n",
    "\n",
    "For a smooth experience, we recommend running the tutorial on PCs equipped with 12th Gen Intel® Core™ processor or higher, and at least 16GB RAM. For server users, we recommend the ones with Intel® Xeon® processors.\n",
    "\n",
    "For OS, BigDL-LLM supports Ubuntu 20.04 or later, CentOS 7 or later, and Windows 10/11.\n",
    "\n",
    "### 1.2 Conda and Environment Management\n",
    "\n",
    "[Conda](https://docs.conda.io/projects/conda/en/stable/) is an open-source package & environment management system which is supported in multiple platforms. It provides a convenient way to manage packages and create isolated environments for different projects. We highly recommend using Conda here to create environment for the tutorials.\n",
    "\n",
    "#### 1.2.1 Install Conda\n",
    "\n",
    "##### 1.2.1.1 Linux\n",
    "For Linux users, you could install Conda through:\n",
    "\n",
    "```bash\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "bash ./Miniconda3-latest-Linux-x86_64.sh\n",
    "```\n",
    "\n",
    "Then you could run:\n",
    "```bash\n",
    "conda init\n",
    "```\n",
    "and follow the output instructions to finish the Conda initialization.\n",
    "\n",
    "\n",
    "##### 1.2.1.2 Native Windows\n",
    "For native Windows users, you could download Conda installer [here](https://docs.conda.io/en/latest/miniconda.html#latest-miniconda-installer-links) based on your system information.\n",
    "\n",
    "After the installation, open \"Anaconda Powershell Prompt (Miniconda3)\" for the following steps.\n",
    "\n",
    "##### 1.2.1.3 Windows with WSL\n",
    "For WSL users, you could follow the same instructions in section [1.2.1.1 Linux](#1211-linux).\n",
    "\n",
    "> **Related Readings**\n",
    ">\n",
    "> For how to install WSL on your windows, refer to [here](https://bigdl.readthedocs.io/en/latest/doc/UserGuide/win.html#install-wsl2) for more information.\n",
    "\n",
    "#### 1.2.2 Create Environment\n",
    "We suggest using Python 3.9 for BigDL-LLM. To create an environment with Python 3.9, run:\n",
    "```\n",
    "conda create -n llm-tutorial python=3.9\n",
    "```\n",
    "You have the flexibility to choose any name you prefer instead of `llm-tutorial`.\n",
    "\n",
    "You can then activate the environment through:\n",
    "```\n",
    "conda activate llm-tutorial\n",
    "```\n",
    "and proceed with the installation of other packages.\n",
    "\n",
    "##### 1.2.2.1 Install Jupyter Notebook\n",
    "Package `notebook` is required to be install for running this tutorial:\n",
    "```\n",
    "pip install notebook\n",
    "```\n",
    "\n",
    "After installation, you could use the following command:\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "to open and run the tutorial notebook in web browser.\n",
    "\n",
    "## 2. BigDL-LLM Installation\n",
    "\n",
    "Install BigDL-LLM through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bigdl-llm[all]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The all option is for installing other required packages by BigDL-LLM.\n",
    "\n",
    "## 3. Building an Application\n",
    "\n",
    "We now have all the prerequisites to build an application for infering on a large language model with BigDL-LLM transformers INT4 optimization. BigDL-LLM offers a Transformers-style API, which ensures that users familiar with Hugging Face transformers can have a unified and consistent experience. \n",
    "\n",
    "### 3.1 Load Model\n",
    "\n",
    "The first step is to import BigDL-LLM, and load the large language model with INT4 optimization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=\"openlm-research/open_llama_3b\",\n",
    "                                             load_in_4bit=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> [`openlm-research/open_llama_3b`](https://huggingface.co/openlm-research/open_llama_3b) is the id for this pretrained model hosted on [huggingface.co](huggingface.co). By executing this `from_pretrained` function, the model will be downloaded to `~/.cache/huggingface` by default, and then be loaded and converted to BigDL-LLM INT4 format implicitly. You could set the environment variable `HF_HOME` to define where you want the model to be downloaded.\n",
    ">\n",
    "> If you have already downloaded the model, you could also specify `pretrained_model_name_or_path` parameter with the corresponding local path.\n",
    "\n",
    "\n",
    "### 3.2 Load Tokenizer\n",
    "\n",
    "The second step is to load the model's corresponding tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(pretrained_model_name_or_path=\"openlm-research/open_llama_3b\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Conduct Inference\n",
    "\n",
    "You could then conduct inference as using normal transformers API with very low latency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "with torch.inference_mode():\n",
    "    prompt = 'Q: What is LLM?\\nA:'\n",
    "    \n",
    "    # tokenize the input prompt from string to token ids\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    st = time.time()\n",
    "    # predict the next 32 tokens based on the input token ids\n",
    "    output = model.generate(input_ids,\n",
    "                            max_new_tokens=32)\n",
    "    end = time.time()\n",
    "    # decode the predicted token ids to output string\n",
    "    output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f'Inference time: {end-st} s')\n",
    "    print('-'*20, 'Prompt', '-'*20)\n",
    "    print(prompt)\n",
    "    print('-'*20, 'Output', '-'*20)\n",
    "    print(output_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> `max_new_tokens` parameter in the `generate` function defines the maximum number of tokens to predict. \n",
    "\n",
    "\n",
    "#### 4. What's Next?\n",
    "\n",
    "In the upcoming chapter, you will dive deeper into the usage of the BigDL-LLM Transformers-style API. The tutorials in the next chapter will also allow you to leverage the power of BigDL-LLM in different domains, such as speech recognition."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
