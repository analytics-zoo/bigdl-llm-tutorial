{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5.1: ChatBot\n",
    "\n",
    "You can use BigDL-LLM to load any Hugging Face *transformers* model for acceleration on your laptop. With BigDL-LLM, PyTorch models (in FP16/BF16/FP32) hosted on Hugging Face can be loaded and optimized automatically with low-bit quantizations (supported precisions include INT4/NF4/INT5/INT8).\n",
    "\n",
    "This notebook will dive into the detailed usage of BigDL-LLM `transformers`-style API. In Sections 5.1.2, you will learn how to load a transformers model for different situations. Section 5.1.3 will walk you through the process of building a chatbot using loaded model. You'll start from a simple form, and then add capabilities step by step, e.g. history management (for multi-turn chat) and streaming.  \n",
    "\n",
    "## 5.1.1 Install BigDL-LLM\n",
    "\n",
    "First of all, install BigDL-LLM in your prepared environment. For best practices of environment setup, refer to [Chapter 2](../ch_2_Environment_Setup/README.md) in this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bigdl-llm[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 Load Model\n",
    "\n",
    "\n",
    "Now let's load the model. We'll use [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) as an example.\n",
    "\n",
    "### 5.1.2.0 Download Llama 2 (7B)\n",
    "\n",
    "To download the [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) model from Hugging Face, you will need to obtain access granted by Meta. Please follow the instructions provided [here](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/tree/main) to request access to the model.\n",
    "\n",
    "After receiving the access, download the model with your Hugging Face token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_path = snapshot_download(repo_id='meta-llama/Llama-2-7b-chat-hf',\n",
    "                              token='hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX') # change it to your own Hugging Face access token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> The model will by default be downloaded to `HF_HOME='~/.cache/huggingface'`.\n",
    "\n",
    "  One common use case is to load a Hugging Face *transformers* model in low precision, i.e. conduct **implicit** quantization while loading.\n",
    "\n",
    "For Llama 2 (7B), you could simply import `bigdl.llm.transformers.AutoModelForCausalLM` instead of `transformers.AutoModelForCausalLM`, and specify `load_in_4bit=True` or `load_in_low_bit` parameter accordingly in the `from_pretrained` function. Compared to the Hugging Face *transformers* API, only minor code changes are required.\n",
    "\n",
    "**For INT4 Optimizations (with `load_in_4bit=True`):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88467d77ffa4f17a423e0339e303d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 09:48:45,428 - INFO - Converting the current model to sym_int4 format......\n"
     ]
    }
   ],
   "source": [
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "\n",
    "model_in_4bit = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=\"../chat-7b-hf/\",\n",
    "                                                     load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> BigDL-LLM has supported `AutoModel`, `AutoModelForCausalLM`, `AutoModelForSpeechSeq2Seq` and `AutoModelForSeq2SeqLM`.\n",
    "\n",
    "**For INT8 Optimizations (with `load_in_low_bit=\"sym_int8\"`):**\n",
    "\n",
    "```python\n",
    "# note that the AutoModelForCausalLM here is imported from bigdl.llm.transformers\n",
    "model_in_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    load_in_low_bit=\"sym_int8\",\n",
    ")\n",
    "```\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> * Currently, `load_in_low_bit` supports options `'sym_int4'`, `'asym_int4'`, `'sym_int5'`, `'asym_int5'` or `'sym_int8'`, in which 'sym' and 'asym' differentiate between symmetric and asymmetric quantization. Option `'nf4'` is also supported, referring to 4-bit NormalFloat.\n",
    ">\n",
    "> *  `load_in_4bit=True` is equivalent to `load_in_low_bit='sym_int4'`.\n",
    "\n",
    "\n",
    "\n",
    "### 5.1.2.2 Load Tokenizer \n",
    "\n",
    "A tokenizer is also needed for LLM inference. It is used to encode input texts to tensors to feed to LLMs, and decode the LLM output tensors to texts. You can use [Huggingface transformers](https://huggingface.co/docs/transformers/index) API to load the tokenizer directly. It can be used seamlessly with models loaded by BigDL-LLM. For Llama 2, the corresponding tokenizer class is `LlamaTokenizer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(pretrained_model_name_or_path=\"../chat-7b-hf/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2.3 Save & Load Low-Precision Model (Optional)\n",
    "\n",
    "`from_pretrained` includes a conversion/quantization step, which can be particularly time-consuming or memory-intensive for some large models. To expedite this process, you can use `save_low_bit` API to store the converted model, after the model is loaded first-time using `from_pretrained`. In subsequent uses, you can opt to use the `load_low_bit` instead of `from_pretrained`, which allows for a direct loading of the pre-converted model and speedup the process. The saving and loading process can be done on different machines.\n",
    "  \n",
    "**Save Low-Precision Model**\n",
    "\n",
    "Let's take the `model_in_4bit` in section 5.1.2.1 as an example. After we loading Llama 2 (7B) in 4 bit, we could use the `save_low_bit` function to save the optimized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory='./llama-2-7b-bigdl-llm-4-bit'\n",
    "\n",
    "model_in_4bit.save_low_bit(save_directory)\n",
    "del(model_in_4bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend saving the tokenizer in the same directory as the optimized model to simplify the subsequent loading process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Low-Precision Model**\n",
    "\n",
    "We could load the optimized low-bit model through `load_low_bit` function, and load tokenizer from the same saved directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 09:56:00,824 - INFO - Converting the current model to sym_int4 format......\n"
     ]
    }
   ],
   "source": [
    "# note that the AutoModelForCausalLM here is imported from bigdl.llm.transformers\n",
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "from transformers import LlamaTokenizer\n",
    "save_directory='./llama-2-7b-bigdl-llm-4-bit'\n",
    "model_in_4bit = AutoModelForCausalLM.load_low_bit(save_directory)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.3 Run Model\n",
    "\n",
    "BigDL-LLM optimized *transformers* model runs much faster than original model. [Chapter 3 Basic Application Develop](../ch_3_AppDev_Basic/) introduces some basics of using optimized model for direct text completion. In this section we will introduce some advanced usages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Einzeln 2018-12-06 at 15:06\n",
      "\n",
      "Artificial intelligence (AI) is a branch of computer science that focuses on creating intelligent machines that can perform tasks that typically require human intelligence, such as understanding natural language, recognizing images, making decisions, and solving problems. AI research involves developing algorithms and statistical models that enable computers to perform these tasks, as well as creating systems that can learn from experience and improve their performance over time.\n",
      "There are several subfields of AI, including:\n",
      "1.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what is AI?\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "# predict next tokens with stopping_criteria\n",
    "output_ids = model_in_4bit.generate(input_ids,\n",
    "                                max_new_tokens=120)\n",
    "output_str = tokenizer.decode(output_ids[0][len(input_ids[0]):], # skip prompt in generated tokens\n",
    "                                  skip_special_tokens=True)\n",
    "\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3.1 Chat\n",
    "\n",
    "One common application of large language models is Chatbot, where LLMs can engage in interactive conversations.\n",
    "Chatbot interaction is no magic - it still relies on the prediction and generation of next tokens by LLMs. \n",
    "To make LLMs chat, we need to properly format the prompts into a converation format, for example:\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant, who always answers as helpfully as possible, while being safe.\n",
    "<</SYS>>\n",
    "\n",
    "What is AI? [/INST]</s>\n",
    "\n",
    "```\n",
    "Further, to enable a multi-turn chat experience, you need to append the new dialog input to the previous conversation to make a new prompt for the model, for example: \n",
    "\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant, who always answers as helpfully as possible, while being safe.\n",
    "<</SYS>>\n",
    "\n",
    "What is AI? [/INST] AI is a term used to describe the development of computer systems that can perform tasks that typically require human intelligence, such as understanding natural language, recognizing images. </s><s> [INST] Is it dangerous? [/INST]\n",
    "```\n",
    "\n",
    "Now we show a multi-turn chat example using official `transformers` API with BigDL-LLM optimized Llama 2 (7B) model. \n",
    "\n",
    "First, define the conversation context format <sup>[1]</sup> for the model to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant.\n",
      "<</SYS>>\n",
      "\n",
      "one plus one? [/INST]\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"You are a helpful, respectful and honest assistant.\"\n",
    "prompt = [f'<s>[INST] <<SYS>>\\n{SYSTEM_PROMPT}\\n<</SYS>>\\n\\n']\n",
    "\n",
    "input_str = \"one plus one?\"\n",
    "input_str = input_str.strip()\n",
    "prompt.append(f'{input_str} [/INST]')\n",
    "prompt = ''.join(prompt)\n",
    "print((prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Of course! 2 + 1 = 3. How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "# predict next tokens with stopping_criteria\n",
    "output_ids = model_in_4bit.generate(input_ids,\n",
    "                                max_new_tokens=120)\n",
    "output_str = tokenizer.decode(output_ids[0][len(input_ids[0]):], # skip prompt in generated tokens\n",
    "                                 skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(input_str, chat_history):\n",
    "    SYSTEM_PROMPT = \"You are a helpful, respectful and honest assistant.\"\n",
    "    prompt = [f'<s>[INST] <<SYS>>\\n{SYSTEM_PROMPT}\\n<</SYS>>\\n\\n']\n",
    "    do_strip = False\n",
    "    for history_input, history_response in chat_history:\n",
    "        history_input = history_input.strip() if do_strip else history_input\n",
    "        do_strip = True\n",
    "        prompt.append(f'{history_input} [/INST] {history_response.strip()} </s><s>[INST] ')\n",
    "    input_str = input_str.strip() if do_strip else input_str\n",
    "    prompt.append(f'{input_str} [/INST]')\n",
    "    #print(''.join(prompt))\n",
    "    return ''.join(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <sup>[1]</sup> The conversation context format is referenced from [here](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat/blob/323df5680706d388eff048fba2f9c9493dfc0152/model.py#L20) and [here](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat/blob/323df5680706d388eff048fba2f9c9493dfc0152/app.py#L9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the `chat` function, which continuously adds model outputs to the chat history. This ensures that conversation context can be properly formatted for next generation of responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(model, tokenizer, input_str, chat_history):\n",
    "    # format conversation context as prompt through chat history\n",
    "    prompt = format_prompt(input_str, chat_history)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # predict next tokens with stopping_criteria\n",
    "    output_ids = model.generate(input_ids,\n",
    "                                max_new_tokens=128)\n",
    "\n",
    "    output_str = tokenizer.decode(output_ids[0][len(input_ids[0]):], # skip prompt in generated tokens\n",
    "                                  skip_special_tokens=True)\n",
    "    print(f\"Response: {output_str.strip()}\")\n",
    "\n",
    "    # add model output to the chat history\n",
    "    chat_history.append((input_str, output_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> BigDL-LLM optimized low-bit models are compatible with all Hugging Face *transformers* APIs. Therefore, in addition to using the `generate` function for token prediction, you can also utilize other methods such as the [`TextGenerationPipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline).\n",
    "\n",
    "Here we go! Let's do interactive, multi-turn chat with LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:hello\n",
      "<s>[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant.\n",
      "<</SYS>>\n",
      "\n",
      "hello [/INST]\n",
      "Response: Hello there! It's nice to meet you. Is there anything I can help you with or any questions you have? I'm here to assist you in any way I can. Please let me know how I can help.\n",
      "Input:one plus one?\n",
      "<s>[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant.\n",
      "<</SYS>>\n",
      "\n",
      "hello [/INST] Hello there! It's nice to meet you. Is there anything I can help you with or any questions you have? I'm here to assist you in any way I can. Please let me know how I can help. </s><s>[INST] one plus one? [/INST]\n",
      "Response: Great, let's do some basic arithmetic! The answer to \"one plus one\" is 2.\n",
      "Input:stop\n",
      "Chat with Llama 2 (7B) stopped.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    with torch.inference_mode():\n",
    "        user_input = input(\"Input:\")\n",
    "        if user_input == \"stop\": # let's stop the conversation when user input \"stop\"\n",
    "            print(\"Chat with Llama 2 (7B) stopped.\")\n",
    "            break\n",
    "        chat(model=model_in_4bit,\n",
    "             tokenizer=tokenizer,\n",
    "             input_str=user_input,\n",
    "             chat_history=chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3.2 Stream Chat\n",
    "\n",
    "Stream chat can be considered as an advanced function for a chatbot, where the response is generated word by word. Here, we define the `stream_chat` function with the help of `transformers.TextIteratorStreamer`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 10:21:49,596 - INFO - Converting the current model to sym_int4 format......\n"
     ]
    }
   ],
   "source": [
    "# note that the AutoModelForCausalLM here is imported from bigdl.llm.transformers\n",
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "from transformers import LlamaTokenizer\n",
    "\n",
    "save_directory='./llama-2-7b-bigdl-llm-4-bit'\n",
    "model_in_4bit = AutoModelForCausalLM.load_low_bit(save_directory)\n",
    "\n",
    "token =  LlamaTokenizer.from_pretrained(save_directory)\n",
    "inputs = token([\"An increasing sequense: one,\"], return_tensors='pt')\n",
    "streamer = TextIteratorStreamer(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <s>An increasing sequense: one, two, three, four, five, six, seven, eight, nine, ten. Unterscheidung between \"one\" and \"on\" is not always clear-cut, but generally \"one\" refers to the number and \"on\" is an adverb meaning \"at or near\". For example: \"Can you pass me one book from the shelf?\" vs. \"The dog is running on the  field.\".</s>"
     ]
    }
   ],
   "source": [
    "generation = dict(inputs, streamer=streamer, max_new_tokens=120)\n",
    "thread = Thread(target=model_in_4bit.generate, kwargs=generation)\n",
    "thread.start()  \n",
    "output_str = []\n",
    "\n",
    "print(\"Response: \", end=\"\")\n",
    "for stream_output in streamer:\n",
    "    output_str.append(stream_output)\n",
    "    print(stream_output, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "def stream_chat(model, tokenizer, input_str, chat_history):\n",
    "    # format conversation context as prompt through chat history\n",
    "    prompt = format_prompt(input_str, chat_history)\n",
    "    input_ids = tokenizer([prompt], return_tensors='pt')\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer,\n",
    "                                    skip_prompt=True, # skip prompt in the generated tokens\n",
    "                                    skip_special_tokens=True)\n",
    "\n",
    "    generate_kwargs = dict(\n",
    "        input_ids,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=128\n",
    "    )\n",
    "    \n",
    "    # to ensure non-blocking access to the generated text, generation process should be ran in a separate thread\n",
    "    from threading import Thread\n",
    "    \n",
    "    thread = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    output_str = []\n",
    "    print(\"Response: \", end=\"\")\n",
    "    for stream_output in streamer:\n",
    "        output_str.append(stream_output)\n",
    "        print(stream_output, end=\"\")\n",
    "\n",
    "    # add model output to the chat history\n",
    "    chat_history.append((input_str, ''.join(output_str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> To successfully observe the text streaming behavior in standard output, we need to set the environment variable `PYTHONUNBUFFERED=1 `to ensure that the standard output streams are directly sent to the terminal without being buffered first.\n",
    ">\n",
    "> The [Hugging Face *transformers* streamer classes](https://huggingface.co/docs/transformers/main/generation_strategies#streaming) is currently being developed and is subject to future changes.\n",
    "\n",
    "We can then achieve interactive, multi-turn stream chat between humans and the bot by allowing continuous user input as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    with torch.inference_mode():\n",
    "        user_input = input(\"Input:\")\n",
    "        if user_input == \"stop\": # let's stop the conversation when user input \"stop\"\n",
    "            print(\"Stream Chat with Llama 2 (7B) stopped.\")\n",
    "            break\n",
    "        stream_chat(model=model_in_4bit,\n",
    "                    tokenizer=tokenizer,\n",
    "                    input_str=user_input,\n",
    "                    chat_history=chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.4 What's Next？\n",
    "\n",
    "In the next tutorial, we will guide you through a speech recognition pipeline that incorporates BigDL-LLM INT4 optimizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
