{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3.1: Run Transformer Models\n",
    "\n",
    "BigDL-LLM supports the optimization of any Hugging Face *transformers* model, allowing for efficient inference with significantly reduced latency. With the help of BigDL-LLM, PyTorch models (in FP16/BF16/FP32) from Hugging Face can be loaded with implicit quantization, so that heavy operations in Transformer can be speeded up through low precision (such as INT4/INT5/INT8, etc.).\n",
    "\n",
    "In this tutorial, we will dive into the main usage of BigDL-LLM Transformers-style API for low-precision optimizations.\n",
    "\n",
    "## 3.1.1 Load Model in INT4 and Conduct Inference\n",
    "\n",
    "One common use case for BigDL-LLM is to load a Hugging Face *transformers* model in INT4 precision and conduct inference with optimizations. Compared to the Hugging Face *transformers* API, only minor code changes are required.\n",
    "\n",
    "For illustration purposes, let's take model [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) as an example.\n",
    "\n",
    "### 3.1.1.1 Download Llama 2 (7B)\n",
    "\n",
    "To download the [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) model from Hugging Face, you will need to obtain access granted by Meta. Please follow the instructions provided [here](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/tree/main) to request access to the model.\n",
    "\n",
    "After receiving the access, download the model with your Hugging Face token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_path = snapshot_download(repo_id='/meta-llama/Llama-2-7b-chat-hf',\n",
    "                               token='hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX') # change it to your own Hugging Face access token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: The model will by default be downloaded to `HF_HOME='~/.cache/huggingface'`.\n",
    "\n",
    "\n",
    "### 3.1.1.2 Load Model in INT4\n",
    "\n",
    "To load the model with BigDL-LLM INT4 optimizations, you could simply import `bigdl.llm.transformers.AutoModelForCausalLM` instead of `transformers.AutoModelForCausalLM`, and specify `load_in_4bit=True` in the `from_pretrained` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                             load_in_4bit=True,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: BigDL-LLM has supported `AutoModel`, `AutoModelForCausalLM`, `AutoModelForSpeechSeq2Seq` and `AutoModelForSeq2SeqLM`.\n",
    "\n",
    "### 3.1.1.3 Load Tokenizer and Conduct Inference\n",
    "\n",
    "You could then load the corresponding tokenizer of Llama 2 (7B) and conduct inference with BigDL-LLM INT4 optimizations, in the exactly same way with using Hugging Face *transformers* API. A human-bot conversation is constructed here for the model to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer of Llama 2 (7B)\n",
    "from transformers import LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(pretrained_model_name_or_path=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "# Generate predicted tokens\n",
    "import torch\n",
    "import time\n",
    "\n",
    "prompt = \"\"\"### HUMAN:\n",
    "What is AI?\n",
    "\n",
    "### RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    st = time.time()\n",
    "    output = model.generate(input_ids,\n",
    "                            max_new_tokens=32)\n",
    "    end = time.time()\n",
    "    output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f'Inference time: {end-st} s')\n",
    "    print('-'*20, 'Prompt', '-'*20)\n",
    "    print(prompt)\n",
    "    print('-'*20, 'Output', '-'*20)\n",
    "    print(output_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2 Save & Load INT4 Model\n",
    "\n",
    "When loading a model with `load_in_4bit=True`, BigDL-LLM implicitly converts linear layers in the model into INT4 format. In theory, a model with *X* B(illion) parameters saved in 16 or 32 bit will requires approximately 2*X* or 4*X* GB of memory for loading. Thus, for extremely large models like the 40B Falcon, 70B Llama 2, 176B Bloom etc., loading them with implicit INT4 quantization of BigDL-LLM can be both resource-intensive and time-consuming, and may even become impossible on memory-limited machines.\n",
    "\n",
    "To address this issue, BigDL-LLM provides support for saving *transformers* models in BigDL-LLM INT4 format. Once the model is optimized and saved in this format, it can be loaded directly for subsequent inference, eliminating the need for repeated quantization. The saving and loading process can be completed on different machines.\n",
    "\n",
    "### 3.1.2.1 Save INT4 Model\n",
    "\n",
    "Let's continue with the example in section 3.1.1 for model Llama 2 (7B). After we loading the model in 4 bit, we could use the `save_low_bit` function to save the optimized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory='./llama-2-7b-bigdl-llm-4-bit'\n",
    "\n",
    "model.save_low_bit(save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend saving the tokenizer in the same directory as the optimized model to simplify the subsequent loading process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2.2 Load INT4 Model\n",
    "\n",
    "We could load the optimized INT4 model through `load_low_bit` function, and load tokenizer from the same saved directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the AutoModelForCausalLM here is imported from bigdl.llm.transformers\n",
    "model_loaded = AutoModelForCausalLM.load_low_bit(save_directory)\n",
    "\n",
    "tokenizer_loaded = LlamaTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference can then be done same as using the Hugging Face *transformers* API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextGenerationPipeline\n",
    "\n",
    "pipeline = TextGenerationPipeline(model=model_loaded, tokenizer=tokenizer_loaded, max_new_tokens=32)\n",
    "output_str = pipeline(prompt)[0][\"generated_text\"]\n",
    "print('-'*20, 'Prompt', '-'*20)\n",
    "print(prompt)\n",
    "print('-'*20, 'Output', '-'*20)\n",
    "print(output_str)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
