{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4.1: Run Transformer Models\n",
    "\n",
    "You can use BigDL-LLM to load any Hugging Face *transformers* model for acceleration on your laptop. With BigDL-LLM, PyTorch models (in FP16/BF16/FP32) hosted on Hugging Face can be loaded and optimized automatically with low-bit quantizations (supported precisions include INT4/INT5/INT8).\n",
    "\n",
    "This notebook will dive into the detailed usage of BigDL-LLM `transformers`-style API. In Sections 4.1.2, you will learn how to load a transformers model for different situations. Section 4.1.3 will walk you through the process of building a chatbot using loaded model. You'll start from a simple form, and then add capabilities step by step, e.g. history management (for multi-turn chat) and streaming.  \n",
    "\n",
    "## 4.1.1 Install BigDL-LLM\n",
    "\n",
    "First of all, install BigDL-LLM in your prepared environment. For best practices of environment setup, refer to [Chapter 2](../ch_2_Environment_Setup/) in this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install BigDL-LLM[all]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2 Load Model\n",
    "\n",
    "\n",
    "Now let's load the model. We'll use [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) as an example.\n",
    "\n",
    "### 4.1.2.0 Download Llama 2 (7B)\n",
    "\n",
    "To download the [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) model from Hugging Face, you will need to obtain access granted by Meta. Please follow the instructions provided [here](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/tree/main) to request access to the model.\n",
    "\n",
    "After receiving the access, download the model with your Hugging Face token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_path = snapshot_download(repo_id='/meta-llama/Llama-2-7b-chat-hf',\n",
    "                               token='hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX') # change it to your own Hugging Face access token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> The model will by default be downloaded to `HF_HOME='~/.cache/huggingface'`.\n",
    "\n",
    "### 4.1.2.1 Load Model in Low Precision\n",
    "\n",
    "One common use case is to load a Hugging Face *transformers* model in low precision, i.e. conduct **implicit** quantization while loading.\n",
    "\n",
    "For Llama 2 (7B), you could simply import `bigdl.llm.transformers.AutoModelForCausalLM` instead of `transformers.AutoModelForCausalLM`, and specify `load_in_4bit=True` or `load_in_low_bit` parameter accordingly in the `from_pretrained` function. Compared to the Hugging Face *transformers* API, only minor code changes are required.\n",
    "\n",
    "**For INT4 Optimizations (with `load_in_4bit=True`):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "\n",
    "model_in_4bit = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                                     load_in_4bit=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> BigDL-LLM has supported `AutoModel`, `AutoModelForCausalLM`, `AutoModelForSpeechSeq2Seq` and `AutoModelForSeq2SeqLM`.\n",
    "\n",
    "**For INT8 Optimizations (with `load_in_low_bit=\"sym_int8\"`):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the AutoModelForCausalLM here is imported from bigdl.llm.transformers\n",
    "model_in_8bit = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                                     load_in_low_bit=\"sym_int8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> * Currently, `load_in_low_bit` supports options `'sym_int4'`, `'asym_int4'`, `'sym_int5'`, `'asym_int5'` or `'sym_int8'`, in which 'sym' and 'asym' differentiate between symmetric and asymmetric quantization.\n",
    ">\n",
    "> *  `load_in_4bit=True` is equivalent to `load_in_low_bit='sym_int4'`.\n",
    "\n",
    "\n",
    "### 4.1.2.2 Load Tokenizer \n",
    "\n",
    "A tokenizer is also needed for LLM inference. It is used to encode input texts to tensors to feed to LLMs, and decode the LLM output tensors to texts. You can use [Huggingface transformers](https://huggingface.co/docs/transformers/index) API to load the tokenizer directly. It can be used seamlessly with models loaded by BigDL-LLM. For Llama 2, the corresponding tokenizer class is `LlamaTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(pretrained_model_name_or_path=\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2.3 Save & Load Low-Precision Model\n",
    "\n",
    "`from_pretrained` includes a conversion/quantization step, which can be particularly time-consuming or memory-intensive for some large models. To expedite this process, you can use `save_low_bit` API to store the converted model, after the model is loaded first-time using `from_pretrained`. In subsequent uses, you can opt to use the `load_low_bit` instead of `from_pretrained`, which allows for a direct loading of the pre-converted model and speedup the process. The saving and loading process can be done on different machines.\n",
    "\n",
    "\n",
    "**Save Low-Precision Model**\n",
    "\n",
    "Let's take the `model_in_4bit` in section 4.1.2.1 as an example. After we loading Llama 2 (7B) in 4 bit, we could use the `save_low_bit` function to save the optimized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory='./llama-2-7b-bigdl-llm-4-bit'\n",
    "\n",
    "model_in_4bit.save_low_bit(save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend saving the tokenizer in the same directory as the optimized model to simplify the subsequent loading process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Low-Precision Model**\n",
    "\n",
    "We could load the optimized low-precision model through `load_low_bit` function, and load tokenizer from the same saved directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the AutoModelForCausalLM here is imported from bigdl.llm.transformers\n",
    "loaded_4bit_model = AutoModelForCausalLM.load_low_bit(save_directory)\n",
    "\n",
    "loaded_tokenizer = LlamaTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3 Run Model\n",
    "\n",
    "BigDL-LLM optimized *transformers* model runs much faster than original model. [Chapter 3 Quick Start](../ch_3_Quick_Start/) introduces some basics of using optimized model for direct text completion. In this section we will introduce some advanced usages.\n",
    "\n",
    "### 4.1.3.1 Chat\n",
    "\n",
    "One common application of large language models is Chatbot, where LLMs can engage in interactive conversations. Chatbot interaction is no magic - it still relies on the prediction and generation of next tokens by LLMs. To make LLMs chat, we need to properly format the prompts into a converation format, for example:\n",
    "\n",
    "```\n",
    "### HUMAN:\n",
    "\n",
    "What is AI?\n",
    "\n",
    "### RESPONSE:\n",
    "```\n",
    "\n",
    "Further, to enable a multi-turn chat experience, you need to append the new dialog input to the previous conversation to make a new prompt for the model, for example: \n",
    "\n",
    "```\n",
    "### HUMAN:\n",
    "\n",
    "What is AI?\n",
    "\n",
    "### RESPONSE:\n",
    "\n",
    "AI is a term used to describe the development of computer systems that can perform tasks that typically require human intelligence, such as understanding natural language, recognizing images.\n",
    "\n",
    "### HUMAN:\n",
    "\n",
    "Is it dangerous?\n",
    "\n",
    "### RESPONSE:\n",
    "```\n",
    "\n",
    "Now we show a multi-turn chat example using official `transformers` API with BigDL-LLM optimized Llama 2 (7B) model. \n",
    "\n",
    "First, define the conversation context format for the model to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN_ID = \"### HUMAN:\\n\\n\"\n",
    "BOT_ID = \"### RESPONSE:\\n\\n\"\n",
    "\n",
    "def format_prompt(input_str, chat_history):\n",
    "    prompt = \"\"\n",
    "    for history_input, history_response in chat_history:\n",
    "      prompt += f\"{HUMAN_ID}{history_input}\\n\\n{BOT_ID}{history_response}\\n\\n\"\n",
    "    prompt += f\"{HUMAN_ID}{input_str}\\n\\n{BOT_ID}\"\n",
    "    return prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define stopping criteria during text generation. This is to avoid Llama 2 (7B) from self-questioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tools.agents import StopSequenceCriteria\n",
    "from transformers.generation.stopping_criteria import StoppingCriteriaList\n",
    "\n",
    "stop_word = \"###\"\n",
    "stopping_criteria = StoppingCriteriaList([StopSequenceCriteria(stop_word, tokenizer)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the `chat` function, which continuously adds model outputs to the chat history. This ensures that conversation context can be properly formatted for next generation of responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(model, tokenizer, input_str, chat_history):\n",
    "    # format conversation context as prompt through chat history\n",
    "    prompt = format_prompt(input_str, chat_history)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # predict next tokens with stopping_criteria\n",
    "    output_ids = model.generate(input_ids,\n",
    "                                max_new_tokens=128,\n",
    "                                stopping_criteria=stopping_criteria)\n",
    "\n",
    "    output_str = tokenizer.decode(output_ids[0][len(input_ids[0]):], # skip prompt in generated tokens\n",
    "                                  skip_special_tokens=True)\n",
    "    print(f\"Response: {output_str.replace(stop_word, '').rstrip()}\")\n",
    "\n",
    "    # add model output to the chat history\n",
    "    chat_history.append((input, output_str.replace(stop_word, \"\").rstrip()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> BigDL-LLM optimized low-precision models are compatible with all Hugging Face *transformers* APIs. Therefore, in addition to using the `generate` function for token prediction, you can also utilize other methods such as the [`TextGenerationPipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline).\n",
    "\n",
    "Here we go! Let's do interactive, multi-turn chat with LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  What is AI?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: AI is a branch of computer science that focuses on creating intelligent machines that can perform tasks that typically require human intelligence, such as understanding natural language, recognizing images, making decisions, and solving problems. AI research involves developing algorithms and models that can learn from data, make predictions, and take actions based on that data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  Is it dangerous?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The potential dangers of AI are a topic of ongoing debate and research. Some concerns include:\n",
      "\n",
      "1. Job displacement: AI could automate many jobs currently performed by humans, leading to job loss and economic disruption.\n",
      "2. Bias and discrimination: AI systems can perpetuate and even amplify existing biases and discrimination if they are trained on biased data.\n",
      "3. Privacy and security risks: AI systems can potentially collect and process large amounts of personal data, which can raise privacy and security concerns.\n",
      "4. Autonomous weapons: The\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with Llama 2 (7B) stopped.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    with torch.inference_mode():\n",
    "        user_input = input(\"Input: \")\n",
    "        if user_input == \"stop\": # let's stop the conversation when user input \"stop\"\n",
    "          print(\"Chat with Llama 2 (7B) stopped.\")\n",
    "          break\n",
    "        chat(model=model_in_4bit,\n",
    "             tokenizer=tokenizer,\n",
    "             input_str=user_input,\n",
    "             chat_history=chat_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3.2 Stream Chat\n",
    "\n",
    "Stream chat can be considered as an advanced function for a chatbot, where the response is generated word by word. Here, we define the `stream_chat` function with the help of `transformers.TextIteratorStreamer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "def stream_chat(model, tokenizer, input_str, chat_history):\n",
    "    # format conversation context as prompt through chat history\n",
    "    prompt = format_prompt(input_str, chat_history)\n",
    "    input_ids = tokenizer([prompt], return_tensors='pt')\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer,\n",
    "                                    skip_prompt=True, # skip prompt in the generated tokens\n",
    "                                    skip_special_tokens=True)\n",
    "\n",
    "    generate_kwargs = dict(\n",
    "        input_ids,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=128,\n",
    "        stopping_criteria=stopping_criteria\n",
    "    )\n",
    "    \n",
    "    # to ensure non-blocking access to the generated text, generation process should be ran in a separate thread\n",
    "    from threading import Thread\n",
    "    \n",
    "    thread = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    output_str = []\n",
    "    print(\"Response: \", end=\"\")\n",
    "    for stream_output in streamer:\n",
    "        output_str.append(stream_output)\n",
    "        print(stream_output.replace(stop_word, \"\"), end=\"\")\n",
    "\n",
    "    # add model output to the chat history\n",
    "    chat_history.append((input, ''.join(output_str).replace(stop_word, \"\").rstrip()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> To successfully observe the text streaming behavior in standard output, we need to set the environment variable `PYTHONUNBUFFERED=1 `to ensure that the standard output streams are directly sent to the terminal without being buffered first.\n",
    ">\n",
    "> The [Hugging Face *transformers* streamer classes](https://huggingface.co/docs/transformers/main/generation_strategies#streaming) is currently being developed and is subject to future changes.\n",
    "\n",
    "We can then achieve interactive, multi-turn stream chat between humans and the bot by allowing continuous user input as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  What is CPU?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: CPU stands for Central Processing Unit. It is the primary component of a computer that performs calculations and executes instructions. The CPU is responsible for fetching instructions from memory, decoding them, executing them, and storing the results. It is the \"brain\" of the computer and performs all the calculations and operations required to run software and applications.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  What is the difference between it and GPU?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The main difference between a CPU and a GPU (Graphics Processing Unit) is their purpose and design. A CPU is designed to perform general-purpose computing tasks, such as running applications, web browsers, and operating systems. It is a \"jack-of-all-trades\" that can perform a wide range of tasks. On the other hand, a GPU is specifically designed to perform complex mathematical operations at high speeds, such as graphics rendering, scientific simulations, and machine learning. It is a \"master-of-one\" that excels at a specific set of tasks.\n",
      "\n",
      "While a CPU can perform"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream Chat with Llama 2 (7B) stopped.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    with torch.inference_mode():\n",
    "        user_input = input(\"Input: \")\n",
    "        if user_input == \"stop\": # let's stop the conversation when user input \"stop\"\n",
    "          print(\"Stream Chat with Llama 2 (7B) stopped.\")\n",
    "          break\n",
    "        stream_chat(model=model_in_4bit,\n",
    "                    tokenizer=tokenizer,\n",
    "                    input_str=user_input,\n",
    "                    chat_history=chat_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.4 What's Next？\n",
    "\n",
    "In the next tutorial, we will guide you through a speech recognition pipeline that incorporates BigDL-LLM INT4 optimizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
